{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tutorial — DATS 6450\n",
    "## Big Data & Cloud Computing\n",
    "\n",
    "**Dataset**: NYC Yellow Taxi Trip Data (public AWS Open Data, `s3://nyc-tlc/`)\n",
    "\n",
    "**Environment**: Single-node t3.large EC2 (2 vCPUs, 8 GB RAM) with `LabInstanceProfile` IAM role attached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Will Learn\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 1 | Environment setup & imports |\n",
    "| 2 | Ray initialization — local vs. cluster mode |\n",
    "| 3 | **Ray Tasks** — parallel fare estimation |\n",
    "| 4 | **Ray Actors** — stateful trip aggregation |\n",
    "| 5 | **Ray Object Store** — zero-copy shared data |\n",
    "| 6 | **Ray Data ETL** — NYC Taxi extract → transform → load |\n",
    "| 7 | Performance: Ray vs. serial pandas |\n",
    "| 8 | Multi-node cluster reference (shown, not executed) |\n",
    "| 9 | Cleanup & summary |\n",
    "\n",
    "### Course Progression\n",
    "\n",
    "```\n",
    "Multiprocessing → Dask → Spark → Ray\n",
    "```\n",
    "\n",
    "Ray differs from Spark in two key ways:\n",
    "- **Stateful actors** — Spark has no equivalent (workers are stateless)\n",
    "- **Async-first** — `.remote()` returns a future immediately; Spark blocks per stage\n",
    "- **Fine-grained tasks** — Ray tasks can be milliseconds; Spark stages are coarser\n",
    "\n",
    "### NYC Taxi Dataset\n",
    "\n",
    "The [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) is hosted publicly on AWS at `s3://nyc-tlc/`. Each Parquet file covers one month of yellow taxi trips (~3M rows, ~50 MB compressed). We use **January 2023** for the ETL demo and three months for the performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 — Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync all dependencies declared in pyproject.toml\n",
    "!uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.fs as pafs\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import time\n",
    "import io\n",
    "\n",
    "print(f\"Ray:      {ray.__version__}\")\n",
    "print(f\"pandas:   {pd.__version__}\")\n",
    "print(f\"PyArrow:  {pa.__version__}\")\n",
    "print(f\"boto3:    {boto3.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the LabInstanceProfile IAM role is attached\n",
    "# (this call succeeds without any credentials in code)\n",
    "s3 = boto3.client(\"s3\")\n",
    "response = s3.list_buckets()\n",
    "buckets = [b[\"Name\"] for b in response[\"Buckets\"]]\n",
    "print(\"Your S3 buckets:\")\n",
    "for b in buckets:\n",
    "    print(f\"  s3://{b}/\")\n",
    "\n",
    "if not buckets:\n",
    "    print(\"No buckets found. Create one in the AWS Console or with:\")\n",
    "    print(\"  aws s3 mb s3://YOUR_NET_ID-dats6450\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✏️  FILL IN YOUR BUCKET NAME (e.g., \"gwu-netid-dats6450\")\n",
    "YOUR_BUCKET_NAME = \"YOUR_BUCKET_NAME_HERE\"\n",
    "\n",
    "assert YOUR_BUCKET_NAME != \"YOUR_BUCKET_NAME_HERE\", \\\n",
    "    \"Please replace YOUR_BUCKET_NAME_HERE with your actual bucket name!\"\n",
    "\n",
    "print(f\"Results will be written to: s3://{YOUR_BUCKET_NAME}/ray-tutorial/results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 — Ray Initialization\n",
    "\n",
    "### Local vs. Cluster Mode\n",
    "\n",
    "| Mode | `ray.init()` call | When to use |\n",
    "|------|-------------------|-------------|\n",
    "| **Local** | `ray.init()` | Development on a single machine |\n",
    "| **Cluster** | `ray.init(address=\"ray://HEAD_IP:10001\")` | Production multi-node cluster |\n",
    "| **Auto-detect** | `ray.init(address=\"auto\")` | When Ray head is already running |\n",
    "\n",
    "The `ignore_reinit_error=True` flag lets you re-run this cell without errors.\n",
    "\n",
    "### Dashboard\n",
    "\n",
    "Ray starts a web dashboard at `http://127.0.0.1:8265`. To access it from your laptop via VSCode Remote-SSH, forward the port:\n",
    "\n",
    "```bash\n",
    "# In VSCode: Ports panel → Add Port → 8265\n",
    "# Or manually:\n",
    "ssh -L 8265:localhost:8265 ubuntu@YOUR_EC2_IP\n",
    "```\n",
    "\n",
    "Then open `http://localhost:8265` in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "print(\"Ray initialized!\")\n",
    "print(f\"Dashboard: http://127.0.0.1:8265\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = ray.available_resources()\n",
    "print(\"Available resources on this node:\")\n",
    "for k, v in sorted(resources.items()):\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nExpected on t3.large: CPU: 2.0, memory: ~7.5 GB\")\n",
    "print(f\"Actual CPUs detected: {resources.get('CPU', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 — Ray Core: Tasks\n",
    "\n",
    "A **Ray Task** is an ordinary Python function decorated with `@ray.remote`. Calling `.remote()` on it:\n",
    "1. Submits the function to the Ray scheduler (non-blocking)\n",
    "2. Returns an **ObjectRef** (a future)\n",
    "3. You collect results with `ray.get()`\n",
    "\n",
    "Tasks are **stateless** — perfect for embarrassingly parallel work like batch transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Motivation: estimate taxi fare for a batch of trips ---\n",
    "# NYC taxi fare formula (simplified):\n",
    "#   base_fare = $2.50\n",
    "#   distance_charge = $0.50 per 1/5 mile = $2.50 per mile\n",
    "#   time_charge = $0.50 per minute in slow traffic\n",
    "\n",
    "def estimate_fare_serial(distance_miles: float, duration_min: float) -> float:\n",
    "    \"\"\"Estimate taxi fare — serial (plain Python).\"\"\"\n",
    "    time.sleep(0.05)  # simulate network I/O or a slow lookup\n",
    "    base_fare = 2.50\n",
    "    distance_charge = distance_miles * 2.50\n",
    "    time_charge = max(0, duration_min - 5) * 0.50  # first 5 min free\n",
    "    return round(base_fare + distance_charge + time_charge, 2)\n",
    "\n",
    "@ray.remote\n",
    "def estimate_fare_remote(distance_miles: float, duration_min: float) -> float:\n",
    "    \"\"\"Estimate taxi fare — Ray Task (runs on a worker process).\"\"\"\n",
    "    time.sleep(0.05)  # same simulated latency\n",
    "    base_fare = 2.50\n",
    "    distance_charge = distance_miles * 2.50\n",
    "    time_charge = max(0, duration_min - 5) * 0.50\n",
    "    return round(base_fare + distance_charge + time_charge, 2)\n",
    "\n",
    "# Sample 40 trips (distance in miles, duration in minutes)\n",
    "np.random.seed(42)\n",
    "n_trips = 40\n",
    "distances = np.random.uniform(0.5, 15.0, n_trips).tolist()\n",
    "durations = np.random.uniform(3.0, 45.0, n_trips).tolist()\n",
    "\n",
    "print(f\"Estimating fares for {n_trips} trips...\")\n",
    "print(f\"Sample trip: {distances[0]:.1f} miles, {durations[0]:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serial baseline\n",
    "t0 = time.time()\n",
    "serial_fares = [estimate_fare_serial(d, m) for d, m in zip(distances, durations)]\n",
    "serial_time = time.time() - t0\n",
    "\n",
    "print(f\"Serial: {serial_time:.2f}s for {n_trips} trips\")\n",
    "print(f\"First 5 fares: {serial_fares[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray parallel version\n",
    "t0 = time.time()\n",
    "\n",
    "# Submit ALL tasks immediately (non-blocking) → list of ObjectRefs\n",
    "futures = [\n",
    "    estimate_fare_remote.remote(d, m)\n",
    "    for d, m in zip(distances, durations)\n",
    "]\n",
    "\n",
    "# Collect results (blocks until all complete)\n",
    "ray_fares = ray.get(futures)\n",
    "ray_time = time.time() - t0\n",
    "\n",
    "print(f\"Ray parallel: {ray_time:.2f}s for {n_trips} trips\")\n",
    "print(f\"Speedup: {serial_time / ray_time:.1f}x\")\n",
    "print(f\"Results match serial: {ray_fares == serial_fares}\")\n",
    "print(f\"\\nWith 2 CPUs and 40 tasks × 50ms each:\")\n",
    "print(f\"  Serial expected: ~{n_trips * 0.05:.1f}s\")\n",
    "print(f\"  Parallel expected: ~{(n_trips / 2) * 0.05:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource annotations: tell Ray how many CPUs a task needs\n",
    "@ray.remote(num_cpus=1)\n",
    "def compute_trip_stats(trip_batch: list) -> dict:\n",
    "    \"\"\"Compute summary stats for a batch of trips.\"\"\"\n",
    "    distances = [t[\"distance\"] for t in trip_batch]\n",
    "    fares = [t[\"fare\"] for t in trip_batch]\n",
    "    return {\n",
    "        \"count\": len(trip_batch),\n",
    "        \"avg_distance\": round(sum(distances) / len(distances), 2),\n",
    "        \"avg_fare\": round(sum(fares) / len(fares), 2),\n",
    "        \"total_revenue\": round(sum(fares), 2),\n",
    "    }\n",
    "\n",
    "# Create 4 batches of 10 trips each\n",
    "trip_data = [\n",
    "    {\"distance\": d, \"fare\": f}\n",
    "    for d, f in zip(distances, serial_fares)\n",
    "]\n",
    "batches = [trip_data[i:i+10] for i in range(0, 40, 10)]\n",
    "\n",
    "# Submit all 4 batch stats computations in parallel\n",
    "stat_refs = [compute_trip_stats.remote(b) for b in batches]\n",
    "stats = ray.get(stat_refs)\n",
    "\n",
    "print(\"Batch stats (4 batches of 10 trips):\")\n",
    "for i, s in enumerate(stats):\n",
    "    print(f\"  Batch {i+1}: {s}\")\n",
    "\n",
    "total_revenue = sum(s[\"total_revenue\"] for s in stats)\n",
    "print(f\"\\nTotal estimated revenue: ${total_revenue:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 — Ray Core: Actors\n",
    "\n",
    "A **Ray Actor** is a Python class decorated with `@ray.remote`. Unlike tasks:\n",
    "- Actors are **stateful** — they maintain state across method calls\n",
    "- Each actor runs in its **own dedicated worker process**\n",
    "- Methods are called with `.remote()` just like tasks\n",
    "\n",
    "**When to use Actors vs. Tasks:**\n",
    "\n",
    "| | Tasks | Actors |\n",
    "|-|-------|--------|\n",
    "| State | Stateless | Stateful |\n",
    "| Use case | Batch transforms, I/O | Counters, caches, ML models |\n",
    "| Scheduling | Any available worker | Pinned to one worker |\n",
    "| Example | `estimate_fare()` | `TripAggregator` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class TripAggregator:\n",
    "    \"\"\"Stateful actor that accumulates trip statistics.\"\"\"\n",
    "\n",
    "    def __init__(self, borough: str):\n",
    "        self.borough = borough\n",
    "        self.trip_count = 0\n",
    "        self.total_fare = 0.0\n",
    "        self.total_distance = 0.0\n",
    "\n",
    "    def record_trip(self, fare: float, distance: float) -> None:\n",
    "        \"\"\"Add a trip's data to the running totals.\"\"\"\n",
    "        self.trip_count += 1\n",
    "        self.total_fare += fare\n",
    "        self.total_distance += distance\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Return summary statistics.\"\"\"\n",
    "        if self.trip_count == 0:\n",
    "            return {\"borough\": self.borough, \"trips\": 0}\n",
    "        return {\n",
    "            \"borough\": self.borough,\n",
    "            \"trips\": self.trip_count,\n",
    "            \"avg_fare\": round(self.total_fare / self.trip_count, 2),\n",
    "            \"avg_distance\": round(self.total_distance / self.trip_count, 2),\n",
    "            \"total_revenue\": round(self.total_fare, 2),\n",
    "        }\n",
    "\n",
    "# Create one actor per borough (each runs in its own worker process)\n",
    "manhattan = TripAggregator.remote(\"Manhattan\")\n",
    "brooklyn = TripAggregator.remote(\"Brooklyn\")\n",
    "queens = TripAggregator.remote(\"Queens\")\n",
    "\n",
    "print(\"Actors created! Each runs in its own dedicated worker process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate streaming trip data to actors\n",
    "# In practice, this could come from a Kafka stream or real-time API\n",
    "np.random.seed(123)\n",
    "n_stream = 100\n",
    "\n",
    "borough_actors = {\"Manhattan\": manhattan, \"Brooklyn\": brooklyn, \"Queens\": queens}\n",
    "boroughs_list = list(borough_actors.keys())\n",
    "\n",
    "# Record trips (all calls are non-blocking — futures are discarded here\n",
    "# because record_trip returns None)\n",
    "record_futures = []\n",
    "for _ in range(n_stream):\n",
    "    borough = np.random.choice(boroughs_list)\n",
    "    fare = np.random.uniform(5, 50)\n",
    "    dist = np.random.uniform(0.5, 12)\n",
    "    future = borough_actors[borough].record_trip.remote(fare, dist)\n",
    "    record_futures.append(future)\n",
    "\n",
    "# Wait for all recording to finish\n",
    "ray.get(record_futures)\n",
    "print(f\"Recorded {n_stream} trips across 3 borough actors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect stats from all actors (in parallel)\n",
    "stats_refs = {b: actor.get_stats.remote() for b, actor in borough_actors.items()}\n",
    "stats_results = {b: ray.get(ref) for b, ref in stats_refs.items()}\n",
    "\n",
    "print(\"Borough trip statistics:\")\n",
    "print(\"-\" * 60)\n",
    "for borough, stats in stats_results.items():\n",
    "    print(f\"  {borough}:\")\n",
    "    print(f\"    Trips: {stats['trips']}\")\n",
    "    print(f\"    Avg fare: ${stats['avg_fare']:.2f}\")\n",
    "    print(f\"    Avg distance: {stats['avg_distance']:.1f} miles\")\n",
    "    print(f\"    Total revenue: ${stats['total_revenue']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 — Ray Core: Object Store\n",
    "\n",
    "Ray's **Object Store** is a shared memory region on each node. When you call `ray.put(data)`, Ray:\n",
    "1. Serializes the object and stores it in shared memory (once)\n",
    "2. Returns an **ObjectRef** — a pointer to the data\n",
    "3. Worker tasks can read the data with **zero-copy** (no deserialization for NumPy/Arrow arrays)\n",
    "\n",
    "This is crucial for large lookup tables, model weights, or reference datasets that many workers need — you avoid the overhead of pickling the data into every task's closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zone lookup table (mimics the NYC Taxi Zone lookup)\n",
    "# In the real dataset this would be loaded from s3://nyc-tlc/misc/taxi_zone_lookup.csv\n",
    "zone_lookup = pd.DataFrame({\n",
    "    \"LocationID\": list(range(1, 266)),\n",
    "    \"Borough\": np.random.choice(\n",
    "        [\"Manhattan\", \"Brooklyn\", \"Queens\", \"Bronx\", \"Staten Island\", \"EWR\"],\n",
    "        265,\n",
    "        p=[0.35, 0.25, 0.20, 0.12, 0.05, 0.03]\n",
    "    ),\n",
    "    \"Zone\": [f\"Zone_{i}\" for i in range(1, 266)],\n",
    "    \"avg_congestion_factor\": np.random.uniform(0.8, 2.5, 265).round(2),\n",
    "})\n",
    "\n",
    "print(f\"Zone lookup table: {len(zone_lookup)} rows, {zone_lookup.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "print(zone_lookup.head(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the lookup table in the Object Store ONCE\n",
    "# All worker tasks will read from this single copy — no re-serialization\n",
    "zone_ref = ray.put(zone_lookup)\n",
    "print(f\"Zone lookup stored in Object Store. ObjectRef: {zone_ref}\")\n",
    "\n",
    "@ray.remote\n",
    "def enrich_trip(trip_id: int, pickup_zone_id: int, dropoff_zone_id: int,\n",
    "                fare: float, zone_lookup_ref) -> dict:\n",
    "    \"\"\"Enrich a trip with zone info from the shared object store.\"\"\"\n",
    "    # Workers access zone_lookup from shared memory — zero-copy for Arrow/pandas\n",
    "    lookup = zone_lookup_ref  # Ray dereferences the ObjectRef automatically\n",
    "\n",
    "    pickup_row = lookup[lookup[\"LocationID\"] == pickup_zone_id]\n",
    "    dropoff_row = lookup[lookup[\"LocationID\"] == dropoff_zone_id]\n",
    "\n",
    "    pickup_borough = pickup_row[\"Borough\"].iloc[0] if len(pickup_row) > 0 else \"Unknown\"\n",
    "    dropoff_borough = dropoff_row[\"Borough\"].iloc[0] if len(dropoff_row) > 0 else \"Unknown\"\n",
    "    congestion = pickup_row[\"avg_congestion_factor\"].iloc[0] if len(pickup_row) > 0 else 1.0\n",
    "\n",
    "    return {\n",
    "        \"trip_id\": trip_id,\n",
    "        \"pickup_borough\": pickup_borough,\n",
    "        \"dropoff_borough\": dropoff_borough,\n",
    "        \"fare\": fare,\n",
    "        \"congestion_adjusted_fare\": round(fare * congestion, 2),\n",
    "    }\n",
    "\n",
    "# Enrich 200 trips — zone_ref is passed by reference, not by value\n",
    "np.random.seed(0)\n",
    "n_enrich = 200\n",
    "enrich_futures = [\n",
    "    enrich_trip.remote(\n",
    "        i,\n",
    "        np.random.randint(1, 266),\n",
    "        np.random.randint(1, 266),\n",
    "        np.random.uniform(5, 50),\n",
    "        zone_ref,  # ← passed as ObjectRef; workers fetch from shared memory\n",
    "    )\n",
    "    for i in range(n_enrich)\n",
    "]\n",
    "\n",
    "enriched = ray.get(enrich_futures)\n",
    "enriched_df = pd.DataFrame(enriched)\n",
    "print(f\"Enriched {n_enrich} trips using shared zone lookup.\")\n",
    "print(enriched_df.head(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3-5 complete — shut down before the ETL section reinitializes\n",
    "ray.shutdown()\n",
    "print(\"Ray Core sections complete. Ray shut down.\")\n",
    "print()\n",
    "print(\"Summary — Ray Core primitives:\")\n",
    "print(\"  Tasks  → @ray.remote on functions  → stateless parallel work\")\n",
    "print(\"  Actors → @ray.remote on classes    → stateful services\")\n",
    "print(\"  Objects → ray.put() / ray.get()    → shared data in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 — Ray Data ETL Pipeline\n",
    "\n",
    "**Ray Data** is Ray's distributed data processing library. It provides:\n",
    "- Lazy, parallel reads from S3, GCS, HDFS, and local filesystem\n",
    "- `map_batches()` for applying Python functions to data in parallel\n",
    "- Streaming execution — data never has to fit in RAM all at once\n",
    "\n",
    "### Pipeline overview\n",
    "\n",
    "```\n",
    "s3://nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\n",
    "         ↓  ray.data.read_parquet()        [Extract]\n",
    "         ↓  map_batches(clean_taxi_batch)   [Transform — clean]\n",
    "         ↓  map_batches(engineer_features)  [Transform — features]\n",
    "         ↓  write_parquet(s3://YOUR_BUCKET) [Load]\n",
    "```\n",
    "\n",
    "> **Note on anonymous S3 access**: The NYC TLC bucket is public. We use `pyarrow.fs.S3FileSystem(anonymous=True)` — this is the most reliable way with Ray 2.x (not the `s3://anonymous@` URL scheme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize Ray for the ETL section\n",
    "ray.init(ignore_reinit_error=True)\n",
    "print(f\"Ray re-initialized. CPUs: {ray.available_resources().get('CPU', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXTRACT ---\n",
    "# Anonymous PyArrow S3 filesystem for public buckets\n",
    "s3_anon = pafs.S3FileSystem(anonymous=True, region=\"us-east-1\")\n",
    "\n",
    "TAXI_PATH = \"nyc-tlc/trip data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "print(f\"Reading: s3://{TAXI_PATH}\")\n",
    "print(\"(This may take 30-60s on first read — Ray downloads and caches the file)\")\n",
    "\n",
    "t_read_start = time.time()\n",
    "ds = ray.data.read_parquet(\n",
    "    TAXI_PATH,\n",
    "    filesystem=s3_anon,\n",
    ")\n",
    "print(f\"Dataset created (lazy) in {time.time() - t_read_start:.2f}s\")\n",
    "print(f\"Schema: {ds.schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXPLORE ---\n",
    "print(\"=== First 5 rows ===\")\n",
    "ds.show(5)\n",
    "\n",
    "# Count rows (triggers actual read)\n",
    "t0 = time.time()\n",
    "row_count = ds.count()\n",
    "print(f\"\\nTotal rows: {row_count:,} (counted in {time.time()-t0:.1f}s)\")\n",
    "\n",
    "# Sample ~10% for quick stats\n",
    "sample_df = ds.limit(300_000).to_pandas()\n",
    "print(f\"\\n=== Quick stats on 300k-row sample ===\")\n",
    "print(sample_df[[\"trip_distance\", \"fare_amount\", \"tip_amount\",\n",
    "                 \"passenger_count\"]].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TRANSFORM: Clean ---\n",
    "def clean_taxi_batch(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove outliers and invalid records.\n",
    "    Ray calls this function on each batch (partition) in parallel.\n",
    "    \"\"\"\n",
    "    # Drop timezone awareness to avoid Ray 2.x timestamp warnings\n",
    "    for col in [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]:\n",
    "        if col in df.columns and hasattr(df[col], \"dt\"):\n",
    "            df[col] = df[col].dt.tz_localize(None)\n",
    "\n",
    "    # Filter: valid fares\n",
    "    df = df[(df[\"fare_amount\"] >= 2.50) & (df[\"fare_amount\"] <= 500)]\n",
    "    # Filter: valid distances\n",
    "    df = df[(df[\"trip_distance\"] > 0) & (df[\"trip_distance\"] <= 100)]\n",
    "    # Filter: valid passenger counts\n",
    "    df = df[(df[\"passenger_count\"] >= 1) & (df[\"passenger_count\"] <= 6)]\n",
    "    # Filter: both pickup and dropoff present\n",
    "    df = df.dropna(subset=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"])\n",
    "    # Filter: dropoff after pickup\n",
    "    df = df[df[\"tpep_dropoff_datetime\"] > df[\"tpep_pickup_datetime\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Applying clean_taxi_batch across all partitions in parallel...\")\n",
    "t0 = time.time()\n",
    "cleaned_ds = ds.map_batches(clean_taxi_batch, batch_format=\"pandas\")\n",
    "cleaned_count = cleaned_ds.count()\n",
    "print(f\"Cleaned in {time.time()-t0:.1f}s\")\n",
    "print(f\"Rows before: {row_count:,}  →  After: {cleaned_count:,}  \"\n",
    "      f\"({100*cleaned_count/row_count:.1f}% retained)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TRANSFORM: Feature Engineering ---\n",
    "def engineer_features_batch(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add derived columns useful for analysis.\n",
    "    Runs in parallel on each partition.\n",
    "    \"\"\"\n",
    "    # Drop tz awareness again (in case this batch comes fresh from file)\n",
    "    for col in [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]:\n",
    "        if hasattr(df[col], \"dt\") and df[col].dt.tz is not None:\n",
    "            df[col] = df[col].dt.tz_localize(None)\n",
    "\n",
    "    # Trip duration in minutes\n",
    "    df[\"trip_duration_min\"] = (\n",
    "        (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"])\n",
    "        .dt.total_seconds() / 60\n",
    "    ).round(2)\n",
    "\n",
    "    # Filter impossibly short/long trips after computing duration\n",
    "    df = df[(df[\"trip_duration_min\"] >= 1) & (df[\"trip_duration_min\"] <= 180)]\n",
    "\n",
    "    # Time features\n",
    "    df[\"pickup_hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "    df[\"pickup_dow\"] = df[\"tpep_pickup_datetime\"].dt.dayofweek  # 0=Mon, 6=Sun\n",
    "\n",
    "    # Tip percentage\n",
    "    df[\"tip_pct\"] = (df[\"tip_amount\"] / df[\"fare_amount\"] * 100).round(1)\n",
    "    df[\"tip_pct\"] = df[\"tip_pct\"].clip(0, 100)\n",
    "\n",
    "    # Average speed (mph)\n",
    "    df[\"avg_speed_mph\"] = (\n",
    "        df[\"trip_distance\"] / (df[\"trip_duration_min\"] / 60)\n",
    "    ).round(1)\n",
    "    df[\"avg_speed_mph\"] = df[\"avg_speed_mph\"].clip(0, 100)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Engineering features in parallel...\")\n",
    "t0 = time.time()\n",
    "featured_ds = cleaned_ds.map_batches(engineer_features_batch, batch_format=\"pandas\")\n",
    "\n",
    "# Preview the engineered features\n",
    "sample = featured_ds.limit(5).to_pandas()\n",
    "print(f\"Features engineered in {time.time()-t0:.1f}s\")\n",
    "new_cols = [\"trip_duration_min\", \"pickup_hour\", \"pickup_dow\", \"tip_pct\", \"avg_speed_mph\"]\n",
    "print(sample[new_cols].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AGGREGATE: Hourly summary ---\n",
    "# Ray Data doesn't have a built-in groupby; we collect to pandas for aggregation\n",
    "# (on large clusters you'd use map_batches + reduce pattern)\n",
    "print(\"Collecting featured dataset to pandas for aggregation...\")\n",
    "print(\"(~30-60s — streams from all partitions)\")\n",
    "t0 = time.time()\n",
    "featured_df = featured_ds.to_pandas()\n",
    "print(f\"Collected {len(featured_df):,} rows in {time.time()-t0:.1f}s\")\n",
    "\n",
    "hourly_summary = (\n",
    "    featured_df\n",
    "    .groupby(\"pickup_hour\")\n",
    "    .agg(\n",
    "        trip_count=(\"trip_duration_min\", \"count\"),\n",
    "        avg_fare=(\"fare_amount\", \"mean\"),\n",
    "        avg_tip_pct=(\"tip_pct\", \"mean\"),\n",
    "        avg_speed=(\"avg_speed_mph\", \"mean\"),\n",
    "        avg_duration=(\"trip_duration_min\", \"mean\"),\n",
    "    )\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    "    .sort_values(\"pickup_hour\")\n",
    ")\n",
    "\n",
    "print(\"\\nHourly summary (first 6 hours):\")\n",
    "print(hourly_summary.head(6).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD: Write to S3 ---\n",
    "print(\"Writing cleaned + featured trips to S3 as Parquet...\")\n",
    "\n",
    "s3_output_path = f\"s3://{YOUR_BUCKET_NAME}/ray-tutorial/results/cleaned_trips/\"\n",
    "\n",
    "# Ray Data write_parquet uses your EC2's IAM role for authentication\n",
    "# No credentials needed in code — LabInstanceProfile handles it\n",
    "t0 = time.time()\n",
    "featured_ds.write_parquet(s3_output_path)\n",
    "print(f\"Written to {s3_output_path} in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Write hourly summary CSV via boto3\n",
    "csv_key = \"ray-tutorial/results/hourly_summary.csv\"\n",
    "csv_buffer = io.StringIO()\n",
    "hourly_summary.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.put_object(\n",
    "    Bucket=YOUR_BUCKET_NAME,\n",
    "    Key=csv_key,\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType=\"text/csv\",\n",
    ")\n",
    "print(f\"Hourly summary CSV written to s3://{YOUR_BUCKET_NAME}/{csv_key}\")\n",
    "print()\n",
    "print(\"Verify with:\")\n",
    "print(f\"  aws s3 ls s3://{YOUR_BUCKET_NAME}/ray-tutorial/results/ --recursive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZE ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(\"NYC Yellow Taxi — January 2023 (Hourly Patterns)\", fontsize=13)\n",
    "\n",
    "hours = hourly_summary[\"pickup_hour\"]\n",
    "\n",
    "# Panel 1: Trips by hour\n",
    "axes[0].bar(hours, hourly_summary[\"trip_count\"] / 1000, color=\"steelblue\", alpha=0.8)\n",
    "axes[0].set_xlabel(\"Pickup Hour\")\n",
    "axes[0].set_ylabel(\"Trips (thousands)\")\n",
    "axes[0].set_title(\"Trip Volume\")\n",
    "axes[0].set_xticks(range(0, 24, 3))\n",
    "\n",
    "# Panel 2: Average fare by hour\n",
    "axes[1].plot(hours, hourly_summary[\"avg_fare\"], color=\"darkorange\",\n",
    "             marker=\"o\", markersize=4, linewidth=2)\n",
    "axes[1].set_xlabel(\"Pickup Hour\")\n",
    "axes[1].set_ylabel(\"Average Fare ($)\")\n",
    "axes[1].set_title(\"Avg Fare by Hour\")\n",
    "axes[1].set_xticks(range(0, 24, 3))\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Average speed by hour\n",
    "axes[2].plot(hours, hourly_summary[\"avg_speed\"], color=\"forestgreen\",\n",
    "             marker=\"s\", markersize=4, linewidth=2)\n",
    "axes[2].set_xlabel(\"Pickup Hour\")\n",
    "axes[2].set_ylabel(\"Avg Speed (mph)\")\n",
    "axes[2].set_title(\"Avg Speed by Hour\")\n",
    "axes[2].set_xticks(range(0, 24, 3))\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"nyc_taxi_hourly.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: nyc_taxi_hourly.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 — Performance Comparison: Ray Data vs. Serial Pandas\n",
    "\n",
    "We process 3 months of NYC Taxi data (Jan–Mar 2023) in two ways:\n",
    "1. **Serial pandas** — read each Parquet file sequentially with PyArrow, apply the same transforms\n",
    "2. **Ray Data** — read all 3 files in parallel with `read_parquet`, apply the same `map_batches` pipeline\n",
    "\n",
    "### Why the speedup may be modest on t3.large\n",
    "\n",
    "- **Only 2 CPUs** — parallelism is limited\n",
    "- **S3 network I/O** is often the bottleneck, not CPU\n",
    "- On a `c5.18xlarge` (72 CPUs) or a 5-node cluster, expect 5–15× speedup for CPU-bound transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three Parquet files we'll process\n",
    "MONTHS = [\"2023-01\", \"2023-02\", \"2023-03\"]\n",
    "PARQUET_PATHS = [\n",
    "    f\"nyc-tlc/trip data/yellow_tripdata_{m}.parquet\"\n",
    "    for m in MONTHS\n",
    "]\n",
    "\n",
    "# --- Serial pandas baseline ---\n",
    "def process_month_pandas(path: str, fs) -> pd.DataFrame:\n",
    "    \"\"\"Read and transform one month of taxi data using pandas.\"\"\"\n",
    "    table = pq.read_table(path, filesystem=fs)\n",
    "    df = table.to_pandas()\n",
    "    df = clean_taxi_batch(df)\n",
    "    df = engineer_features_batch(df)\n",
    "    return df\n",
    "\n",
    "print(\"Running serial pandas baseline (3 months)...\")\n",
    "t_serial_start = time.time()\n",
    "\n",
    "serial_dfs = []\n",
    "for path in PARQUET_PATHS:\n",
    "    print(f\"  Reading {path.split('/')[-1]}...\", end=\"\", flush=True)\n",
    "    t0 = time.time()\n",
    "    df = process_month_pandas(path, s3_anon)\n",
    "    serial_dfs.append(df)\n",
    "    print(f\" {len(df):,} rows in {time.time()-t0:.1f}s\")\n",
    "\n",
    "serial_total = pd.concat(serial_dfs, ignore_index=True)\n",
    "t_serial = time.time() - t_serial_start\n",
    "\n",
    "print(f\"\\nSerial total: {len(serial_total):,} rows in {t_serial:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ray Data parallel ---\n",
    "print(\"Running Ray Data parallel (3 months)...\")\n",
    "t_ray_start = time.time()\n",
    "\n",
    "ray_ds = ray.data.read_parquet(PARQUET_PATHS, filesystem=s3_anon)\n",
    "ray_ds = ray_ds.map_batches(clean_taxi_batch, batch_format=\"pandas\")\n",
    "ray_ds = ray_ds.map_batches(engineer_features_batch, batch_format=\"pandas\")\n",
    "\n",
    "ray_total = ray_ds.to_pandas()\n",
    "t_ray = time.time() - t_ray_start\n",
    "\n",
    "print(f\"Ray Data total: {len(ray_total):,} rows in {t_ray:.1f}s\")\n",
    "print(f\"\\nSpeedup: {t_serial / t_ray:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing comparison chart\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "methods = [\"Serial\\n(pandas)\", f\"Ray Data\\n(parallel)\"]\n",
    "times = [t_serial, t_ray]\n",
    "colors = [\"#d62728\", \"#1f77b4\"]\n",
    "\n",
    "bars = ax.bar(methods, times, color=colors, alpha=0.85, width=0.5)\n",
    "\n",
    "for bar, t in zip(bars, times):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
    "            f\"{t:.1f}s\", ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "speedup = t_serial / t_ray\n",
    "ax.set_ylabel(\"Wall-clock time (seconds)\", fontsize=11)\n",
    "ax.set_title(\n",
    "    f\"Ray Data vs. Serial pandas — 3 months NYC Taxi\\n\"\n",
    "    f\"({speedup:.2f}× speedup on t3.large, 2 CPUs)\",\n",
    "    fontsize=11\n",
    ")\n",
    "ax.set_ylim(0, max(times) * 1.25)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Annotation explaining the modest speedup\n",
    "ax.text(0.98, 0.95,\n",
    "        f\"t3.large: 2 CPUs\\nS3 I/O often limits speedup\\nExpect 5–15× on c5.18xlarge\",\n",
    "        transform=ax.transAxes, ha=\"right\", va=\"top\",\n",
    "        fontsize=9, color=\"gray\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ray_vs_pandas_timing.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: ray_vs_pandas_timing.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the speedup matters at scale\n",
    "\n",
    "On a **t3.large** (2 CPUs), Ray's overhead often competes with the parallelism benefit for S3-bound workloads. The real payoff comes when:\n",
    "\n",
    "| Scenario | Expected speedup |\n",
    "|----------|----------------|\n",
    "| t3.large, 2 CPUs, S3 I/O bound | 1–2× |\n",
    "| c5.4xlarge, 16 CPUs, CPU-bound transforms | 4–10× |\n",
    "| 5-node cluster (t3.large × 5), 10 CPUs total | 3–8× |\n",
    "| c5.18xlarge, 72 CPUs, pure CPU transforms | 15–30× |\n",
    "\n",
    "The transform functions (`clean_taxi_batch`, `engineer_features_batch`) are CPU-bound. On a bigger machine or cluster, each additional CPU linearly reduces wall time — up to S3 bandwidth limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8 — Multi-Node Cluster Reference\n",
    "\n",
    "### Why go multi-node?\n",
    "\n",
    "- **More CPUs** → more parallelism in `map_batches`\n",
    "- **More RAM** → larger datasets without spilling to disk\n",
    "- **Distributed Object Store** → each node has its own store; Ray routes data automatically\n",
    "\n",
    "**Cost estimate** (AWS `us-east-1`, on-demand):\n",
    "\n",
    "| Configuration | CPUs | Cost/hr |\n",
    "|---------------|------|--------|\n",
    "| 1× t3.large (current) | 2 | $0.083 |\n",
    "| 3× t3.large cluster | 6 | $0.25 |\n",
    "| 5× t3.large cluster | 10 | $0.42 |\n",
    "\n",
    "> **AWS Academy note**: Multi-node clusters can burn through credits quickly. Always run `ray down` when finished, then stop your EC2 instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE ONLY — do NOT execute this cell; it is shown for illustration\n",
    "# This YAML would be saved as ray-cluster.yaml and used with `ray up`\n",
    "\n",
    "RAY_CLUSTER_YAML = \"\"\"\n",
    "# ray-cluster.yaml — 1 head + 4 workers (5 × t3.large)\n",
    "cluster_name: dats6450-ray\n",
    "\n",
    "provider:\n",
    "  type: aws\n",
    "  region: us-east-1\n",
    "  availability_zone: us-east-1a\n",
    "\n",
    "auth:\n",
    "  ssh_user: ubuntu\n",
    "  ssh_private_key: ~/.ssh/your-key.pem  # your Lab 3a key pair\n",
    "\n",
    "head_node_type: ray.head.default\n",
    "worker_node_types:\n",
    "  - name: ray.worker.default\n",
    "    min_workers: 4\n",
    "    max_workers: 4\n",
    "\n",
    "available_node_types:\n",
    "  ray.head.default:\n",
    "    resources: {}\n",
    "    node_config:\n",
    "      InstanceType: t3.large\n",
    "      ImageId: ami-0c55b159cbfafe1f0  # Ubuntu 22.04 us-east-1\n",
    "      IamInstanceProfile:\n",
    "        Name: LabInstanceProfile\n",
    "      SecurityGroupIds:\n",
    "        - sg-XXXXXXXX  # your Lab 3a security group\n",
    "\n",
    "  ray.worker.default:\n",
    "    resources: {}\n",
    "    node_config:\n",
    "      InstanceType: t3.large\n",
    "      ImageId: ami-0c55b159cbfafe1f0\n",
    "      IamInstanceProfile:\n",
    "        Name: LabInstanceProfile\n",
    "      SecurityGroupIds:\n",
    "        - sg-XXXXXXXX\n",
    "\n",
    "setup_commands:\n",
    "  - pip install -U \"ray[data,serve,tune]>=2.54.0\" pyarrow boto3\n",
    "\n",
    "head_setup_commands:\n",
    "  - pip install jupyter\n",
    "\"\"\"\n",
    "\n",
    "print(RAY_CLUSTER_YAML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster lifecycle commands\n",
    "\n",
    "> **REFERENCE ONLY** — do not run these in this notebook. They would be run in a terminal.\n",
    "\n",
    "```bash\n",
    "# 1. Start the cluster (provisions EC2 instances + installs Ray)\n",
    "ray up ray-cluster.yaml\n",
    "\n",
    "# 2. Check cluster status\n",
    "ray status\n",
    "\n",
    "# 3. Submit a job to the cluster\n",
    "ray job submit --working-dir . \\\n",
    "  --address ray://HEAD_IP:10001 \\\n",
    "  -- python my_ray_script.py\n",
    "\n",
    "# 4. Port-forward the dashboard\n",
    "ray dashboard ray-cluster.yaml\n",
    "# Then open http://localhost:8265\n",
    "\n",
    "# 5. IMPORTANT: Tear down cluster when done to stop billing!\n",
    "ray down ray-cluster.yaml\n",
    "\n",
    "# 6. Connect an existing script to the cluster\n",
    "# Inside the script:\n",
    "#   ray.init(address=\"ray://HEAD_IP:10001\")\n",
    "```\n",
    "\n",
    "When connecting with `ray.init(address=\"ray://HEAD_IP:10001\")`, your `@ray.remote` tasks and actors automatically distribute across all 5 nodes. No other code changes needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9 — Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Ray\n",
    "ray.shutdown()\n",
    "print(\"Ray shut down.\")\n",
    "\n",
    "# List files written to S3\n",
    "print(f\"\\nFiles written to s3://{YOUR_BUCKET_NAME}/ray-tutorial/\")\n",
    "try:\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=YOUR_BUCKET_NAME,\n",
    "        Prefix=\"ray-tutorial/\",\n",
    "    )\n",
    "    if \"Contents\" in response:\n",
    "        for obj in response[\"Contents\"]:\n",
    "            size_kb = obj[\"Size\"] / 1024\n",
    "            print(f\"  {obj['Key']:60s} {size_kb:8.1f} KB\")\n",
    "    else:\n",
    "        print(\"  (no files found — check YOUR_BUCKET_NAME)\")\n",
    "except Exception as e:\n",
    "    print(f\"  Could not list S3 files: {e}\")\n",
    "\n",
    "# List local files\n",
    "import os\n",
    "local_files = [\"nyc_taxi_hourly.png\", \"ray_vs_pandas_timing.png\"]\n",
    "print(\"\\nLocal files created:\")\n",
    "for f in local_files:\n",
    "    if os.path.exists(f):\n",
    "        size_kb = os.path.getsize(f) / 1024\n",
    "        print(f\"  {f:40s} {size_kb:.1f} KB\")\n",
    "    else:\n",
    "        print(f\"  {f:40s} (not found — did Section 6/7 complete?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | What it is | When to use it |\n",
    "|---------|-----------|----------------|\n",
    "| **Ray Task** | `@ray.remote` function → called with `.remote()` | Stateless parallel work (batch transforms, I/O) |\n",
    "| **Ray Actor** | `@ray.remote` class → stateful worker process | Counters, caches, ML model servers |\n",
    "| **Object Store** | `ray.put()` / `ray.get()` — shared memory | Large reference data shared across many tasks |\n",
    "| **Ray Data** | Distributed dataset with lazy `map_batches` | ETL pipelines, ML preprocessing at scale |\n",
    "| **Multi-node** | `ray up` cluster YAML + `ray.init(address=...)` | Production workloads needing >2 CPUs or >8 GB RAM |\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- **Ray Train** — distributed ML training (PyTorch, XGBoost)\n",
    "- **Ray Tune** — hyperparameter optimization at scale\n",
    "- **Ray Serve** — model serving with autoscaling\n",
    "- [Ray documentation](https://docs.ray.io/en/latest/) | [NYC Taxi dataset](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
