{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting NYC Taxi Fares with XGBoost + Ray Train\n",
    "## DATS 6450 — Big Data & Cloud Computing\n",
    "\n",
    "**Prerequisites**: Complete `tutorial.ipynb` first — this notebook assumes Ray, the NYC Taxi dataset, and your S3 bucket are already familiar.\n",
    "\n",
    "**What you will build**: A gradient-boosted tree model that predicts taxi `fare_amount` from trip features — trained in parallel using **Ray Train**.\n",
    "\n",
    "**Environment**: t3.large EC2 (2 vCPUs, 8 GB RAM) with `LabInstanceProfile` attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Will Learn\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 1 | Environment setup & imports |\n",
    "| 2 | Data loading & feature engineering with Ray Data |\n",
    "| 3 | Exploratory analysis — what drives fares? |\n",
    "| 4 | Baseline model — serial XGBoost on a single node |\n",
    "| 5 | Distributed training with **Ray Train + XGBoost** |\n",
    "| 6 | Hyperparameter tuning with **Ray Tune** |\n",
    "| 7 | Model evaluation — errors, feature importance, residuals |\n",
    "| 8 | Saving the model to S3 & batch prediction |\n",
    "| 9 | Cleanup & summary |\n",
    "\n",
    "### Why XGBoost for fare prediction?\n",
    "\n",
    "- **Tabular data** — tree models consistently outperform neural networks on structured data like this\n",
    "- **Mixed feature types** — XGBoost handles integers (hour, zones), floats (distance), and categoricals naturally\n",
    "- **Interpretable** — feature importance tells us which inputs matter most\n",
    "- **Ray Train integration** — `xgboost-ray` lets the same XGBoost model train across multiple Ray workers with zero code changes to the model itself\n",
    "\n",
    "### Target variable\n",
    "\n",
    "We predict **`fare_amount`** — the metered fare set by the TLC rate formula. We intentionally exclude `tip_amount`, `tolls_amount`, and `total_amount` since those are downstream of the fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 — Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train import ScalingConfig, RunConfig, CheckpointConfig\n",
    "import ray.tune as tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.fs as pafs\n",
    "import pyarrow.parquet as pq\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Ray:        {ray.__version__}\")\n",
    "print(f\"XGBoost:    {xgb.__version__}\")\n",
    "print(f\"pandas:     {pd.__version__}\")\n",
    "print(f\"scikit-learn: imported OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✏️  FILL IN YOUR BUCKET NAME (same one used in tutorial.ipynb)\n",
    "YOUR_BUCKET_NAME = \"YOUR_BUCKET_NAME_HERE\"\n",
    "\n",
    "assert YOUR_BUCKET_NAME != \"YOUR_BUCKET_NAME_HERE\", \\\n",
    "    \"Please replace YOUR_BUCKET_NAME_HERE with your actual bucket name!\"\n",
    "\n",
    "MODEL_S3_PREFIX = f\"s3://{YOUR_BUCKET_NAME}/ray-tutorial/models/\"\n",
    "print(f\"Models will be saved to: {MODEL_S3_PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "print(f\"Ray initialized. Resources: {ray.available_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 — Data Loading & Feature Engineering\n",
    "\n",
    "We reuse the same cleaning and feature-engineering logic from `tutorial.ipynb`, applied here with Ray Data across **3 months** of NYC Taxi data (Jan–Mar 2023, ~9M raw rows).\n",
    "\n",
    "### Feature design\n",
    "\n",
    "| Feature | Type | Rationale |\n",
    "|---------|------|----------|\n",
    "| `trip_distance` | float | Primary fare driver (metered by distance) |\n",
    "| `trip_duration_min` | float | Secondary driver (metered by time in traffic) |\n",
    "| `pickup_hour` | int | Rush-hour surcharges, demand pricing |\n",
    "| `pickup_dow` | int | Weekday vs. weekend patterns |\n",
    "| `PULocationID` | int | Origin zone — airport trips have flat fares |\n",
    "| `DOLocationID` | int | Destination zone |\n",
    "| `passenger_count` | int | Minor effect; kept for completeness |\n",
    "| `RatecodeID` | int | Critical: 1=standard, 2=JFK flat ($70), 3=Newark, 5=negotiated |\n",
    "\n",
    "> **Key insight**: `RatecodeID` is the single most predictive feature — JFK flat-rate trips will be ~$70 regardless of distance. The model should learn this quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_anon = pafs.S3FileSystem(anonymous=True, region=\"us-east-1\")\n",
    "\n",
    "MONTHS = [\"2023-01\", \"2023-02\", \"2023-03\"]\n",
    "PARQUET_PATHS = [\n",
    "    f\"nyc-tlc/trip data/yellow_tripdata_{m}.parquet\"\n",
    "    for m in MONTHS\n",
    "]\n",
    "\n",
    "# Feature columns we want to keep (plus the target)\n",
    "FEATURE_COLS = [\n",
    "    \"trip_distance\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"passenger_count\",\n",
    "    \"RatecodeID\",\n",
    "]\n",
    "ENGINEERED_COLS = [\"trip_duration_min\", \"pickup_hour\", \"pickup_dow\"]\n",
    "TARGET = \"fare_amount\"\n",
    "\n",
    "ALL_NEEDED = FEATURE_COLS + ENGINEERED_COLS + [TARGET,\n",
    "    \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]  # needed to engineer features\n",
    "\n",
    "print(f\"Loading {len(MONTHS)} months of NYC Taxi data from S3...\")\n",
    "print(f\"Paths: {[p.split('/')[-1] for p in PARQUET_PATHS]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_model(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combined clean + feature-engineer + select pass.\n",
    "    Runs in parallel on each Ray Data partition.\n",
    "    \"\"\"\n",
    "    # --- Clean timestamps ---\n",
    "    for col in [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]:\n",
    "        if col in df.columns and hasattr(df[col], \"dt\"):\n",
    "            if df[col].dt.tz is not None:\n",
    "                df[col] = df[col].dt.tz_localize(None)\n",
    "\n",
    "    # --- Filter outliers ---\n",
    "    df = df[(df[\"fare_amount\"] >= 2.50) & (df[\"fare_amount\"] <= 300)]\n",
    "    df = df[(df[\"trip_distance\"] > 0) & (df[\"trip_distance\"] <= 60)]\n",
    "    df = df[(df[\"passenger_count\"] >= 1) & (df[\"passenger_count\"] <= 6)]\n",
    "    df = df.dropna(subset=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "                            \"RatecodeID\", \"PULocationID\", \"DOLocationID\"])\n",
    "    df = df[df[\"tpep_dropoff_datetime\"] > df[\"tpep_pickup_datetime\"]]\n",
    "\n",
    "    # --- Engineered features ---\n",
    "    df[\"trip_duration_min\"] = (\n",
    "        (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"])\n",
    "        .dt.total_seconds() / 60\n",
    "    )\n",
    "    df = df[(df[\"trip_duration_min\"] >= 1) & (df[\"trip_duration_min\"] <= 180)]\n",
    "\n",
    "    df[\"pickup_hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "    df[\"pickup_dow\"]  = df[\"tpep_pickup_datetime\"].dt.dayofweek\n",
    "\n",
    "    # --- Cast types for XGBoost ---\n",
    "    int_cols = [\"PULocationID\", \"DOLocationID\", \"passenger_count\",\n",
    "                \"RatecodeID\", \"pickup_hour\", \"pickup_dow\"]\n",
    "    for c in int_cols:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "    # --- Select only the columns the model needs ---\n",
    "    keep = FEATURE_COLS + ENGINEERED_COLS + [TARGET]\n",
    "    return df[keep]\n",
    "\n",
    "\n",
    "print(\"Building Ray Data pipeline...\")\n",
    "t0 = time.time()\n",
    "\n",
    "raw_ds = ray.data.read_parquet(PARQUET_PATHS, filesystem=s3_anon)\n",
    "model_ds = raw_ds.map_batches(prepare_for_model, batch_format=\"pandas\")\n",
    "\n",
    "# Materialize — triggers all the reading and transforms\n",
    "print(\"Materializing dataset (reading S3 + applying transforms)...\")\n",
    "print(\"This takes ~2–4 min on t3.large for 3 months.\")\n",
    "model_df = model_ds.to_pandas()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone in {elapsed:.0f}s\")\n",
    "print(f\"Dataset: {len(model_df):,} rows × {len(model_df.columns)} columns\")\n",
    "print(f\"Memory: {model_df.memory_usage(deep=True).sum() / 1e6:.0f} MB\")\n",
    "print(f\"\\nColumns: {list(model_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset summary:\")\n",
    "print(model_df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 — Exploratory Analysis: What Drives Fares?\n",
    "\n",
    "Before training, let's look at the relationships between features and `fare_amount`. This guides feature selection and gives us a sanity check that the data looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on a 50k sample for fast plotting\n",
    "sample = model_df.sample(50_000, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "fig.suptitle(\"NYC Taxi — Feature Relationships with Fare Amount\", fontsize=13)\n",
    "\n",
    "# 1. Distance vs. Fare (the primary relationship)\n",
    "ax = axes[0, 0]\n",
    "ax.hexbin(sample[\"trip_distance\"], sample[\"fare_amount\"],\n",
    "          gridsize=50, cmap=\"Blues\", mincnt=1)\n",
    "ax.set_xlabel(\"Trip Distance (miles)\")\n",
    "ax.set_ylabel(\"Fare Amount ($)\")\n",
    "ax.set_title(\"Distance vs. Fare\")\n",
    "ax.set_xlim(0, 30)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# 2. Duration vs. Fare\n",
    "ax = axes[0, 1]\n",
    "ax.hexbin(sample[\"trip_duration_min\"], sample[\"fare_amount\"],\n",
    "          gridsize=50, cmap=\"Greens\", mincnt=1)\n",
    "ax.set_xlabel(\"Trip Duration (min)\")\n",
    "ax.set_ylabel(\"Fare Amount ($)\")\n",
    "ax.set_title(\"Duration vs. Fare\")\n",
    "ax.set_xlim(0, 60)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# 3. Fare distribution by RatecodeID\n",
    "ax = axes[0, 2]\n",
    "ratecode_labels = {1: \"Standard\", 2: \"JFK\", 3: \"Newark\", 4: \"Nassau\", 5: \"Negotiated\", 6: \"Group\"}\n",
    "for rc in sorted(sample[\"RatecodeID\"].unique()):\n",
    "    if rc in ratecode_labels:\n",
    "        subset = sample[sample[\"RatecodeID\"] == rc][\"fare_amount\"]\n",
    "        ax.hist(subset.clip(0, 120), bins=40, alpha=0.5,\n",
    "                label=f\"{rc}: {ratecode_labels.get(rc, rc)}\", density=True)\n",
    "ax.set_xlabel(\"Fare Amount ($)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Fare Distribution by Rate Code\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# 4. Avg fare by pickup hour\n",
    "ax = axes[1, 0]\n",
    "hourly = model_df.groupby(\"pickup_hour\")[\"fare_amount\"].mean()\n",
    "ax.plot(hourly.index, hourly.values, color=\"darkorange\", marker=\"o\", markersize=4)\n",
    "ax.set_xlabel(\"Pickup Hour\")\n",
    "ax.set_ylabel(\"Avg Fare ($)\")\n",
    "ax.set_title(\"Avg Fare by Hour\")\n",
    "ax.set_xticks(range(0, 24, 3))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Avg fare by day of week\n",
    "ax = axes[1, 1]\n",
    "dow_labels = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "daily = model_df.groupby(\"pickup_dow\")[\"fare_amount\"].mean()\n",
    "ax.bar(dow_labels, daily.values, color=\"steelblue\", alpha=0.8)\n",
    "ax.set_ylabel(\"Avg Fare ($)\")\n",
    "ax.set_title(\"Avg Fare by Day of Week\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 6. Fare distribution (log scale)\n",
    "ax = axes[1, 2]\n",
    "ax.hist(model_df[\"fare_amount\"].clip(0, 150), bins=80,\n",
    "        color=\"mediumslateblue\", alpha=0.8, edgecolor=\"none\")\n",
    "ax.set_xlabel(\"Fare Amount ($)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Fare Amount Distribution\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.axvline(model_df[\"fare_amount\"].median(), color=\"red\",\n",
    "           linestyle=\"--\", linewidth=1.5, label=f\"Median: ${model_df['fare_amount'].median():.2f}\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fare_eda.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: fare_eda.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix — numeric features only\n",
    "corr_cols = FEATURE_COLS + ENGINEERED_COLS + [TARGET]\n",
    "corr = model_df[corr_cols].corr()[[TARGET]].drop(TARGET).sort_values(TARGET, ascending=False)\n",
    "\n",
    "print(\"Pearson correlation with fare_amount:\")\n",
    "print(corr.round(3).to_string())\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  • trip_distance has the strongest linear correlation\")\n",
    "print(\"  • trip_duration_min is also strong (correlated with distance)\")\n",
    "print(\"  • RatecodeID is moderate — but nonlinear (JFK flat rate = $70)\")\n",
    "print(\"  • pickup_hour/dow have weak linear correlation but nonlinear patterns\")\n",
    "print(\"  → XGBoost captures all nonlinear interactions automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 — Baseline: Serial XGBoost on a Single Node\n",
    "\n",
    "Before using Ray Train, we establish a **serial baseline** — standard XGBoost on a 500k-row sample. This:\n",
    "1. Confirms the model works before adding distributed complexity\n",
    "2. Gives us a timing baseline to compare against Ray Train\n",
    "3. Shows us what a reasonable MAE / RMSE looks like\n",
    "\n",
    "### Train / validation / test split strategy\n",
    "\n",
    "We split **temporally** rather than randomly — this avoids leakage and simulates real deployment where the model is trained on past data and evaluated on future data.\n",
    "\n",
    "```\n",
    "January 2023    →  Training set   (60%)\n",
    "February 2023   →  Validation set (20%)   ← used during tuning\n",
    "March 2023      →  Test set       (20%)   ← final evaluation only\n",
    "```\n",
    "\n",
    "For the serial baseline we sample 500k rows from the full dataset for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS_MODEL = FEATURE_COLS + ENGINEERED_COLS\n",
    "\n",
    "# Sample 500k rows for the serial baseline (fast iteration)\n",
    "baseline_df = model_df.sample(500_000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = baseline_df[FEATURE_COLS_MODEL]\n",
    "y = baseline_df[TARGET]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train):,} rows\")\n",
    "print(f\"Val:   {len(X_val):,} rows\")\n",
    "print(f\"Test:  {len(X_test):,} rows\")\n",
    "print(f\"Features: {FEATURE_COLS_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serial XGBoost baseline\n",
    "baseline_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"tree_method\": \"hist\",   # fast histogram-based algorithm\n",
    "    \"n_jobs\": -1,            # use all CPUs on the node\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "print(\"Training serial XGBoost baseline...\")\n",
    "t0 = time.time()\n",
    "\n",
    "baseline_model = xgb.XGBRegressor(**baseline_params)\n",
    "baseline_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=50,\n",
    ")\n",
    "\n",
    "baseline_time = time.time() - t0\n",
    "print(f\"\\nTraining complete in {baseline_time:.1f}s\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "mae_b  = mean_absolute_error(y_test, y_pred_baseline)\n",
    "rmse_b = mean_squared_error(y_test, y_pred_baseline, squared=False)\n",
    "r2_b   = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"\\nBaseline test-set metrics (500k sample):\")\n",
    "print(f\"  MAE:  ${mae_b:.2f}\")\n",
    "print(f\"  RMSE: ${rmse_b:.2f}\")\n",
    "print(f\"  R²:   {r2_b:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 — Distributed Training with Ray Train + XGBoost\n",
    "\n",
    "**Ray Train** wraps XGBoost's native distributed training. Under the hood:\n",
    "1. Ray Train spawns `num_workers` Ray actors\n",
    "2. Each actor gets a shard of the data\n",
    "3. XGBoost's distributed mode (using Rabit — a reduction ring) synchronizes gradient updates across workers\n",
    "4. The result is identical to single-node XGBoost — just faster when you have many CPUs/nodes\n",
    "\n",
    "### Key Ray Train concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| `XGBoostTrainer` | Ray Train's XGBoost integration — wraps `xgb.train()` |\n",
    "| `ScalingConfig` | How many workers, CPUs per worker |\n",
    "| `RunConfig` | Where to save checkpoints and logs |\n",
    "| `ray.data.Dataset` | Input data — Ray Train reads from this natively |\n",
    "\n",
    "> **On t3.large (2 CPUs)**: Ray Train with 2 workers gives similar speed to serial XGBoost with `n_jobs=-1`. The architecture benefit shows on multi-node clusters — `ScalingConfig(num_workers=10)` on a 5-node cluster would use all 10 CPUs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our pandas DataFrame back to a Ray Dataset\n",
    "# Ray Train reads directly from Ray Datasets (no manual sharding needed)\n",
    "print(\"Converting pandas DataFrame to Ray Dataset...\")\n",
    "\n",
    "train_df = model_df.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n",
    "\n",
    "n_total = len(train_df)\n",
    "n_train = int(n_total * 0.70)\n",
    "n_val   = int(n_total * 0.15)\n",
    "\n",
    "train_split = train_df.iloc[:n_train]\n",
    "val_split   = train_df.iloc[n_train:n_train + n_val]\n",
    "test_split  = train_df.iloc[n_train + n_val:]\n",
    "\n",
    "print(f\"Full dataset split:\")\n",
    "print(f\"  Train: {len(train_split):,} rows\")\n",
    "print(f\"  Val:   {len(val_split):,} rows\")\n",
    "print(f\"  Test:  {len(test_split):,} rows\")\n",
    "\n",
    "# Convert to Ray Datasets\n",
    "train_ray_ds = ray.data.from_pandas(train_split)\n",
    "val_ray_ds   = ray.data.from_pandas(val_split)\n",
    "test_ray_ds  = ray.data.from_pandas(test_split)\n",
    "\n",
    "print(f\"\\nRay Dataset schema: {train_ray_ds.schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBoost parameters for Ray Train\n",
    "# These are identical to what you'd pass to xgb.train() directly\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": [\"rmse\", \"mae\"],\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,                 # learning rate (called 'eta' in xgb.train API)\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# ScalingConfig: how Ray distributes the training\n",
    "# num_workers=2 → 2 Ray actors, each getting half the data\n",
    "# On t3.large we have 2 vCPUs, so 1 CPU per worker\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=2,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "# RunConfig: where to save results\n",
    "run_config = RunConfig(\n",
    "    name=\"xgboost_fare_prediction\",\n",
    "    storage_path=os.path.expanduser(\"~/ray_results\"),\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=1,  # save only the best checkpoint\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Ray Train config ready.\")\n",
    "print(f\"  Workers: {scaling_config.num_workers}\")\n",
    "print(f\"  XGBoost params: {xgb_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the XGBoostTrainer\n",
    "# label_column → column in the Ray Dataset to use as the target\n",
    "# datasets → dict mapping split names to Ray Datasets\n",
    "trainer = XGBoostTrainer(\n",
    "    params=xgb_params,\n",
    "    label_column=TARGET,\n",
    "    num_boost_round=200,\n",
    "    datasets={\n",
    "        \"train\": train_ray_ds,\n",
    "        \"valid\": val_ray_ds,\n",
    "    },\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "print(\"Starting Ray Train XGBoost distributed training...\")\n",
    "print(f\"Training on {len(train_split):,} rows with {scaling_config.num_workers} workers\")\n",
    "print(\"(Expect ~3–6 min on t3.large)\")\n",
    "\n",
    "t0 = time.time()\n",
    "result = trainer.fit()\n",
    "ray_train_time = time.time() - t0\n",
    "\n",
    "print(f\"\\nTraining complete in {ray_train_time:.1f}s\")\n",
    "print(f\"Best checkpoint: {result.checkpoint}\")\n",
    "print(f\"\\nFinal metrics:\")\n",
    "for k, v in result.metrics.items():\n",
    "    if \"rmse\" in k or \"mae\" in k:\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the Ray Train checkpoint\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "\n",
    "checkpoint = result.checkpoint\n",
    "\n",
    "# Load model for evaluation\n",
    "predictor = XGBoostPredictor.from_checkpoint(checkpoint)\n",
    "\n",
    "# Predict on test set (using the Ray Dataset)\n",
    "print(\"Running predictions on test set with Ray Train predictor...\")\n",
    "test_predictions_ds = predictor.predict(test_ray_ds.drop_columns([TARGET]))\n",
    "test_preds_df = test_predictions_ds.to_pandas()\n",
    "\n",
    "# Align predictions with actuals\n",
    "y_test_vals = test_split[TARGET].values\n",
    "y_pred_ray  = test_preds_df[\"predictions\"].values\n",
    "\n",
    "mae_ray  = mean_absolute_error(y_test_vals, y_pred_ray)\n",
    "rmse_ray = mean_squared_error(y_test_vals, y_pred_ray, squared=False)\n",
    "r2_ray   = r2_score(y_test_vals, y_pred_ray)\n",
    "\n",
    "print(f\"\\nRay Train XGBoost — test-set metrics (full {len(test_split):,}-row test set):\")\n",
    "print(f\"  MAE:  ${mae_ray:.2f}\")\n",
    "print(f\"  RMSE: ${rmse_ray:.2f}\")\n",
    "print(f\"  R²:   {r2_ray:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 — Hyperparameter Tuning with Ray Tune\n",
    "\n",
    "**Ray Tune** automates hyperparameter search. It integrates with Ray Train — each trial is a full training run, and Tune manages scheduling, early stopping, and result logging.\n",
    "\n",
    "### ASHA Scheduler\n",
    "\n",
    "We use **Asynchronous Successive Halving (ASHA)** — an early-stopping algorithm that:\n",
    "1. Starts many trials in parallel with a short `num_boost_round`\n",
    "2. Keeps only the top-performing trials and gives them more rounds\n",
    "3. Eliminates poor trials early — far more efficient than grid search\n",
    "\n",
    "### Search space\n",
    "\n",
    "We tune the four parameters with the most impact on XGBoost fare prediction:\n",
    "\n",
    "| Parameter | Effect |\n",
    "|-----------|--------|\n",
    "| `max_depth` | Tree complexity — deeper = more expressive, but overfits |\n",
    "| `eta` | Learning rate — smaller = more rounds needed, but smoother |\n",
    "| `subsample` | Row subsampling — reduces variance |\n",
    "| `colsample_bytree` | Feature subsampling per tree — reduces correlation between trees |\n",
    "\n",
    "> **Note**: On t3.large, we run a small search (4 trials, 100 rounds max) to keep it within ~10 minutes. On a larger instance or cluster, increase `num_samples` to 20–50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a 300k-row subset for tuning (faster iteration per trial)\n",
    "tune_df    = train_df.sample(300_000, random_state=0).reset_index(drop=True)\n",
    "n_tune     = len(tune_df)\n",
    "n_tune_tr  = int(n_tune * 0.80)\n",
    "\n",
    "tune_train_ds = ray.data.from_pandas(tune_df.iloc[:n_tune_tr])\n",
    "tune_val_ds   = ray.data.from_pandas(tune_df.iloc[n_tune_tr:])\n",
    "\n",
    "print(f\"Tuning on {n_tune_tr:,} train / {n_tune - n_tune_tr:,} val rows\")\n",
    "\n",
    "# Search space — Ray Tune samples from these distributions\n",
    "param_space = {\n",
    "    \"params\": {\n",
    "        \"objective\":        \"reg:squarederror\",\n",
    "        \"eval_metric\":      [\"rmse\"],\n",
    "        \"tree_method\":      \"hist\",\n",
    "        \"seed\":             42,\n",
    "        \"max_depth\":        tune.randint(3, 10),\n",
    "        \"eta\":              tune.loguniform(0.01, 0.3),\n",
    "        \"subsample\":        tune.uniform(0.6, 1.0),\n",
    "        \"colsample_bytree\": tune.uniform(0.5, 1.0),\n",
    "        \"min_child_weight\": tune.randint(1, 20),\n",
    "    },\n",
    "    \"label_column\": TARGET,\n",
    "    \"num_boost_round\": 100,\n",
    "    \"datasets\": {\n",
    "        \"train\": tune_train_ds,\n",
    "        \"valid\": tune_val_ds,\n",
    "    },\n",
    "    \"scaling_config\": ScalingConfig(num_workers=2, use_gpu=False),\n",
    "}\n",
    "\n",
    "# ASHA Scheduler — aggressive early stopping\n",
    "scheduler = ASHAScheduler(\n",
    "    time_attr=\"training_iteration\",\n",
    "    max_t=100,           # max boosting rounds per trial\n",
    "    grace_period=20,     # minimum rounds before stopping a trial\n",
    "    reduction_factor=2,\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter search space defined.\")\n",
    "print(\"Starting Ray Tune search (4 trials × 100 rounds max)...\")\n",
    "print(\"(~5–10 min on t3.large)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    XGBoostTrainer,\n",
    "    param_space=param_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"valid-rmse\",\n",
    "        mode=\"min\",\n",
    "        scheduler=scheduler,\n",
    "        num_samples=4,       # number of trials (increase on bigger instances)\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        name=\"xgb_fare_tune\",\n",
    "        storage_path=os.path.expanduser(\"~/ray_results\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "tune_results = tuner.fit()\n",
    "tune_time = time.time() - t0\n",
    "\n",
    "print(f\"\\nTuning complete in {tune_time:.0f}s\")\n",
    "\n",
    "# Show all trial results\n",
    "results_df = tune_results.get_dataframe()\n",
    "cols_to_show = [\"valid-rmse\",\n",
    "                \"config/params/max_depth\", \"config/params/eta\",\n",
    "                \"config/params/subsample\", \"config/params/colsample_bytree\",\n",
    "                \"config/params/min_child_weight\"]\n",
    "available_cols = [c for c in cols_to_show if c in results_df.columns]\n",
    "print(\"\\nTrial results (sorted by val RMSE):\")\n",
    "print(results_df[available_cols].sort_values(\"valid-rmse\").round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best trial\n",
    "best_result = tune_results.get_best_result(metric=\"valid-rmse\", mode=\"min\")\n",
    "best_params = best_result.config[\"params\"]\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "for k, v in best_params.items():\n",
    "    if k not in [\"objective\", \"eval_metric\", \"tree_method\", \"seed\"]:\n",
    "        print(f\"  {k}: {v}\")\n",
    "print(f\"  → val RMSE: {best_result.metrics.get('valid-rmse', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain on the full training set with the best hyperparameters\n",
    "print(\"Retraining with best params on full training set...\")\n",
    "\n",
    "final_trainer = XGBoostTrainer(\n",
    "    params=best_params,\n",
    "    label_column=TARGET,\n",
    "    num_boost_round=200,    # more rounds now that params are tuned\n",
    "    datasets={\n",
    "        \"train\": train_ray_ds,\n",
    "        \"valid\": val_ray_ds,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=False),\n",
    "    run_config=RunConfig(\n",
    "        name=\"xgb_fare_final\",\n",
    "        storage_path=os.path.expanduser(\"~/ray_results\"),\n",
    "        checkpoint_config=CheckpointConfig(num_to_keep=1),\n",
    "    ),\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "final_result = final_trainer.fit()\n",
    "print(f\"Final training complete in {time.time()-t0:.0f}s\")\n",
    "\n",
    "# Evaluate the tuned model\n",
    "final_predictor = XGBoostPredictor.from_checkpoint(final_result.checkpoint)\n",
    "final_preds_ds  = final_predictor.predict(test_ray_ds.drop_columns([TARGET]))\n",
    "final_preds_df  = final_preds_ds.to_pandas()\n",
    "\n",
    "y_pred_tuned = final_preds_df[\"predictions\"].values\n",
    "\n",
    "mae_tuned  = mean_absolute_error(y_test_vals, y_pred_tuned)\n",
    "rmse_tuned = mean_squared_error(y_test_vals, y_pred_tuned, squared=False)\n",
    "r2_tuned   = r2_score(y_test_vals, y_pred_tuned)\n",
    "\n",
    "print(f\"\\nTuned model — test-set metrics:\")\n",
    "print(f\"  MAE:  ${mae_tuned:.2f}  (baseline: ${mae_b:.2f})\")\n",
    "print(f\"  RMSE: ${rmse_tuned:.2f}  (baseline: ${rmse_b:.2f})\")\n",
    "print(f\"  R²:   {r2_tuned:.4f}  (baseline: {r2_b:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 — Model Evaluation\n",
    "\n",
    "Three complementary views of model quality:\n",
    "1. **Actual vs. Predicted** — how well-calibrated is the model?\n",
    "2. **Residual analysis** — are errors systematic (suggesting missing features)?\n",
    "3. **Feature importance** — which features drive the predictions most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a 10k subsample for plotting clarity\n",
    "plot_idx   = np.random.default_rng(0).integers(0, len(y_test_vals), 10_000)\n",
    "y_actual   = y_test_vals[plot_idx]\n",
    "y_pred_plot = y_pred_tuned[plot_idx]\n",
    "residuals  = y_actual - y_pred_plot\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle(\"Tuned XGBoost — Fare Prediction Evaluation\", fontsize=13)\n",
    "\n",
    "# --- Panel 1: Actual vs. Predicted ---\n",
    "ax = axes[0]\n",
    "lim = (0, 80)\n",
    "ax.hexbin(y_actual, y_pred_plot, gridsize=50, cmap=\"viridis\",\n",
    "          extent=(*lim, *lim), mincnt=1)\n",
    "ax.plot(lim, lim, \"r--\", linewidth=1.5, label=\"Perfect prediction\")\n",
    "ax.set_xlabel(\"Actual Fare ($)\")\n",
    "ax.set_ylabel(\"Predicted Fare ($)\")\n",
    "ax.set_title(f\"Actual vs. Predicted\\nR² = {r2_tuned:.4f}\")\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# --- Panel 2: Residuals vs. Predicted ---\n",
    "ax = axes[1]\n",
    "ax.hexbin(y_pred_plot, residuals, gridsize=50, cmap=\"RdYlBu\",\n",
    "          extent=(0, 80, -30, 30), mincnt=1)\n",
    "ax.axhline(0, color=\"black\", linewidth=1.5, linestyle=\"--\")\n",
    "ax.axhline(mae_tuned, color=\"red\", linewidth=1, linestyle=\":\",\n",
    "           label=f\"+MAE = ${mae_tuned:.2f}\")\n",
    "ax.axhline(-mae_tuned, color=\"red\", linewidth=1, linestyle=\":\")\n",
    "ax.set_xlabel(\"Predicted Fare ($)\")\n",
    "ax.set_ylabel(\"Residual (Actual − Predicted)\")\n",
    "ax.set_title(f\"Residuals\\nMAE = ${mae_tuned:.2f}, RMSE = ${rmse_tuned:.2f}\")\n",
    "ax.set_xlim(0, 80)\n",
    "ax.set_ylim(-30, 30)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# --- Panel 3: Residual distribution ---\n",
    "ax = axes[2]\n",
    "ax.hist(residuals.clip(-25, 25), bins=60, color=\"steelblue\",\n",
    "        alpha=0.8, edgecolor=\"none\", density=True)\n",
    "ax.axvline(0, color=\"black\", linewidth=1.5, linestyle=\"--\")\n",
    "ax.axvline(np.mean(residuals), color=\"red\", linewidth=1.5,\n",
    "           label=f\"Mean error: ${np.mean(residuals):.2f}\")\n",
    "ax.set_xlabel(\"Residual ($)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Residual Distribution\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_evaluation.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: model_evaluation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the underlying xgb.Booster from the checkpoint for feature importance\n",
    "# (XGBoostPredictor wraps it; we can get it with get_model())\n",
    "booster = final_predictor.get_model()\n",
    "\n",
    "# Feature importance — three views\n",
    "importance_types = [\"weight\", \"gain\", \"cover\"]\n",
    "importance_labels = {\n",
    "    \"weight\": \"Frequency\\n(# times used in splits)\",\n",
    "    \"gain\":   \"Gain\\n(avg improvement per split)\",\n",
    "    \"cover\":  \"Cover\\n(avg # samples per split)\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle(\"XGBoost Feature Importance — NYC Taxi Fare Prediction\", fontsize=13)\n",
    "\n",
    "for ax, imp_type in zip(axes, importance_types):\n",
    "    scores = booster.get_score(importance_type=imp_type)\n",
    "    # Map feature names (f0, f1...) back to column names\n",
    "    feat_names = FEATURE_COLS_MODEL\n",
    "    named_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        # XGBoost uses 'f0', 'f1' etc. when trained via Ray Train\n",
    "        idx_str = k.replace(\"f\", \"\")\n",
    "        if idx_str.isdigit() and int(idx_str) < len(feat_names):\n",
    "            named_scores[feat_names[int(idx_str)]] = v\n",
    "        else:\n",
    "            named_scores[k] = v\n",
    "\n",
    "    if named_scores:\n",
    "        sorted_scores = sorted(named_scores.items(), key=lambda x: x[1])\n",
    "        features, vals = zip(*sorted_scores)\n",
    "        ax.barh(features, vals, color=\"steelblue\", alpha=0.8)\n",
    "        ax.set_title(f\"{imp_type.capitalize()} Importance\")\n",
    "        ax.set_xlabel(importance_labels[imp_type])\n",
    "        ax.grid(axis=\"x\", alpha=0.3)\n",
    "    else:\n",
    "        ax.set_title(f\"{imp_type.capitalize()} (unavailable)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error breakdown by rate code — are we worse for special fares?\n",
    "test_eval_df = test_split.copy()\n",
    "test_eval_df[\"predicted_fare\"] = y_pred_tuned\n",
    "test_eval_df[\"abs_error\"]      = np.abs(y_pred_tuned - y_test_vals)\n",
    "\n",
    "rc_labels = {1: \"Standard\", 2: \"JFK\", 3: \"Newark\", 4: \"Nassau\",\n",
    "             5: \"Negotiated\", 6: \"Group\"}\n",
    "\n",
    "rc_errors = (\n",
    "    test_eval_df\n",
    "    .groupby(\"RatecodeID\")\n",
    "    .agg(\n",
    "        n=(\"abs_error\", \"count\"),\n",
    "        mae=(\"abs_error\", \"mean\"),\n",
    "        avg_fare=(\"fare_amount\", \"mean\"),\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "rc_errors[\"label\"] = rc_errors.index.map(rc_labels)\n",
    "rc_errors[\"pct_error\"] = (rc_errors[\"mae\"] / rc_errors[\"avg_fare\"] * 100).round(1)\n",
    "\n",
    "print(\"MAE by Rate Code:\")\n",
    "print(rc_errors[[\"label\", \"n\", \"avg_fare\", \"mae\", \"pct_error\"]].to_string())\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  • RatecodeID=2 (JFK flat rate ~$70) should have very low MAE\")\n",
    "print(\"  • RatecodeID=5 (Negotiated) may be higher — those fares are unpredictable\")\n",
    "print(\"  • High pct_error for small rate codes (3, 4, 6) is expected with few samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8 — Saving the Model to S3 & Batch Prediction\n",
    "\n",
    "### Workflow\n",
    "\n",
    "```\n",
    "Ray Train checkpoint  →  extract xgb.Booster  →  pickle  →  s3://YOUR_BUCKET/models/\n",
    "                                                             ↓\n",
    "New trip data  →  Ray Data map_batches(predict_batch)  →  predictions Parquet on S3\n",
    "```\n",
    "\n",
    "In production you'd use **Ray Serve** to wrap this into a REST endpoint, but for this tutorial we demonstrate offline batch prediction — a common pattern for nightly fare estimation or analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the booster to S3 ---\n",
    "model_bytes = pickle.dumps(booster)\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "model_key  = \"ray-tutorial/models/xgb_fare_model.pkl\"\n",
    "\n",
    "s3_client.put_object(\n",
    "    Bucket=YOUR_BUCKET_NAME,\n",
    "    Key=model_key,\n",
    "    Body=model_bytes,\n",
    "    ContentType=\"application/octet-stream\",\n",
    ")\n",
    "print(f\"Model saved to s3://{YOUR_BUCKET_NAME}/{model_key}\")\n",
    "print(f\"Model size: {len(model_bytes) / 1024:.1f} KB\")\n",
    "\n",
    "# Also save in XGBoost's native format (JSON — more portable)\n",
    "booster.save_model(\"/tmp/xgb_fare_model.json\")\n",
    "with open(\"/tmp/xgb_fare_model.json\", \"rb\") as f:\n",
    "    s3_client.put_object(\n",
    "        Bucket=YOUR_BUCKET_NAME,\n",
    "        Key=\"ray-tutorial/models/xgb_fare_model.json\",\n",
    "        Body=f.read(),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "print(f\"Native JSON model saved to s3://{YOUR_BUCKET_NAME}/ray-tutorial/models/xgb_fare_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Batch prediction with Ray Data ---\n",
    "# Simulate \"new\" trips — April 2023 data\n",
    "print(\"Loading April 2023 data for batch prediction...\")\n",
    "\n",
    "APRIL_PATH = \"nyc-tlc/trip data/yellow_tripdata_2023-04.parquet\"\n",
    "april_raw  = ray.data.read_parquet(APRIL_PATH, filesystem=s3_anon)\n",
    "april_ds   = april_raw.map_batches(prepare_for_model, batch_format=\"pandas\")\n",
    "\n",
    "# Sample 100k rows for a fast demo\n",
    "april_sample_ds = april_ds.limit(100_000)\n",
    "print(f\"April sample dataset ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch-predict using map_batches — each worker loads the model once\n",
    "# We use ray.put() to share the model bytes across workers (Object Store pattern!)\n",
    "model_bytes_ref = ray.put(model_bytes)\n",
    "\n",
    "def predict_batch(df: pd.DataFrame, model_ref) -> pd.DataFrame:\n",
    "    \"\"\"Load model from Object Store and predict fares for a batch.\"\"\"\n",
    "    import pickle\n",
    "    import xgboost as xgb\n",
    "\n",
    "    booster = pickle.loads(model_ref)\n",
    "    features = [c for c in FEATURE_COLS_MODEL if c in df.columns]\n",
    "    dmatrix = xgb.DMatrix(df[features])\n",
    "    df[\"predicted_fare\"] = booster.predict(dmatrix).round(2)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Running batch predictions on April 2023 data...\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Drop the target column (it's in our dataset; in production it wouldn't exist)\n",
    "features_only_ds = april_sample_ds.drop_columns([TARGET])\n",
    "\n",
    "predictions_ds = features_only_ds.map_batches(\n",
    "    predict_batch,\n",
    "    batch_format=\"pandas\",\n",
    "    fn_kwargs={\"model_ref\": model_bytes_ref},\n",
    ")\n",
    "\n",
    "# Write predictions to S3\n",
    "preds_s3_path = f\"s3://{YOUR_BUCKET_NAME}/ray-tutorial/results/april_predictions/\"\n",
    "predictions_ds.write_parquet(preds_s3_path)\n",
    "\n",
    "print(f\"Batch predictions written to {preds_s3_path}\")\n",
    "print(f\"Elapsed: {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Show a sample\n",
    "sample_preds = predictions_ds.limit(10).to_pandas()\n",
    "print(\"\\nSample predictions:\")\n",
    "print(sample_preds[[\"trip_distance\", \"RatecodeID\", \"pickup_hour\",\n",
    "                     \"predicted_fare\"]].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predicted vs. actual for April (we do have the actual fares)\n",
    "april_with_actual_ds = april_sample_ds\n",
    "april_preds_ds = april_with_actual_ds.map_batches(\n",
    "    predict_batch,\n",
    "    batch_format=\"pandas\",\n",
    "    fn_kwargs={\"model_ref\": model_bytes_ref},\n",
    ")\n",
    "april_eval_df = april_preds_ds.to_pandas()\n",
    "\n",
    "mae_april  = mean_absolute_error(april_eval_df[TARGET], april_eval_df[\"predicted_fare\"])\n",
    "rmse_april = mean_squared_error(april_eval_df[TARGET], april_eval_df[\"predicted_fare\"],\n",
    "                                squared=False)\n",
    "r2_april   = r2_score(april_eval_df[TARGET], april_eval_df[\"predicted_fare\"])\n",
    "\n",
    "print(\"Out-of-time evaluation — April 2023 (model trained on Jan–Mar):\")\n",
    "print(f\"  MAE:  ${mae_april:.2f}\")\n",
    "print(f\"  RMSE: ${rmse_april:.2f}\")\n",
    "print(f\"  R²:   {r2_april:.4f}\")\n",
    "print()\n",
    "print(\"If April metrics are similar to the test set → no data drift.\")\n",
    "print(\"If significantly worse → fare structure may have changed (e.g. rate hike).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9 — Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "print(\"Ray shut down.\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 62)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 62)\n",
    "print(f\"{'Model':<35} {'MAE ($)':>8} {'RMSE ($)':>9} {'R²':>7}\")\n",
    "print(\"-\" * 62)\n",
    "print(f\"{'Serial XGBoost (500k sample)':<35} {mae_b:>8.2f} {rmse_b:>9.2f} {r2_b:>7.4f}\")\n",
    "print(f\"{'Ray Train XGBoost (full, default params)':<35} {mae_ray:>8.2f} {rmse_ray:>9.2f} {r2_ray:>7.4f}\")\n",
    "print(f\"{'Ray Train XGBoost (tuned)':<35} {mae_tuned:>8.2f} {rmse_tuned:>9.2f} {r2_tuned:>7.4f}\")\n",
    "print(f\"{'April 2023 (out-of-time)':<35} {mae_april:>8.2f} {rmse_april:>9.2f} {r2_april:>7.4f}\")\n",
    "print(\"=\" * 62)\n",
    "\n",
    "# List S3 outputs\n",
    "print(f\"\\nFiles written to s3://{YOUR_BUCKET_NAME}/ray-tutorial/\")\n",
    "try:\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    resp = s3_client.list_objects_v2(Bucket=YOUR_BUCKET_NAME, Prefix=\"ray-tutorial/models/\")\n",
    "    for obj in resp.get(\"Contents\", []):\n",
    "        print(f\"  {obj['Key']:60s} {obj['Size']/1024:8.1f} KB\")\n",
    "    resp2 = s3_client.list_objects_v2(\n",
    "        Bucket=YOUR_BUCKET_NAME, Prefix=\"ray-tutorial/results/april_predictions/\"\n",
    "    )\n",
    "    for obj in resp2.get(\"Contents\", [])[:5]:\n",
    "        print(f\"  {obj['Key']:60s} {obj['Size']/1024:8.1f} KB\")\n",
    "except Exception as e:\n",
    "    print(f\"  Could not list: {e}\")\n",
    "\n",
    "# Local files\n",
    "print(\"\\nLocal files created:\")\n",
    "for f in [\"fare_eda.png\", \"model_evaluation.png\", \"feature_importance.png\"]:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"  {f:40s} {os.path.getsize(f)/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What you built\n",
    "\n",
    "| Step | Tool | What happened |\n",
    "|------|------|---------------|\n",
    "| Data loading | `ray.data.read_parquet` | Read 3 months (~9M rows) from S3 in parallel |\n",
    "| Cleaning & features | `map_batches` | Parallel pandas transforms across all partitions |\n",
    "| Baseline model | `xgb.XGBRegressor` | Serial XGBoost on 500k sample |\n",
    "| Distributed training | `XGBoostTrainer` + `ScalingConfig` | Ray spawns workers, data shards automatically |\n",
    "| Hyperparameter tuning | `ray.tune.Tuner` + `ASHAScheduler` | 4 trials with early stopping |\n",
    "| Evaluation | sklearn metrics + matplotlib | MAE, RMSE, R², feature importance |\n",
    "| Model persistence | `boto3.put_object` | Saved to S3 as `.pkl` and `.json` |\n",
    "| Batch prediction | `map_batches` + `ray.put()` | Model in Object Store → parallel predict → Parquet on S3 |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "A MAE of ~$2–3 means the model is off by about **1 stop's worth** of metered fare on average. The key drivers in order are:\n",
    "\n",
    "1. **`RatecodeID`** — flat-rate codes (JFK=$70, Newark) dominate once learned\n",
    "2. **`trip_distance`** — $0.50 per 1/5 mile in the meter\n",
    "3. **`trip_duration_min`** — slow traffic charges\n",
    "4. **`PULocationID` / `DOLocationID`** — zone-level patterns (airports, bridges)\n",
    "5. **`pickup_hour`** — very mild effect; TLC fare formula doesn't have a surge multiplier\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- **Ray Serve** — wrap the XGBoost model in a REST API endpoint (real-time predictions)\n",
    "- **More features** — weather data (open-meteo API), events calendar, holiday flags\n",
    "- **Multi-month training** — add 2022 data; compare seasonal drift\n",
    "- **SHAP values** — `shap` library + XGBoost for per-prediction explanation\n",
    "- [Ray Train docs](https://docs.ray.io/en/latest/train/train.html) | [Ray Tune docs](https://docs.ray.io/en/latest/tune/index.html) | [XGBoost-Ray](https://docs.ray.io/en/latest/train/examples/xgboost/xgboost-ray-example.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
